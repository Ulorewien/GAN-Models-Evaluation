{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561},{"sourceId":4607952,"sourceType":"datasetVersion","datasetId":2604803},{"sourceId":4418815,"sourceType":"datasetVersion","datasetId":2490389}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nfrom datetime import datetime\nimport pandas as pd\nfrom PIL import Image\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as torch_utils\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom weights initialization\n# Reference (PyTorch Tutorials)\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\ndef save_images(images, output_dir):\n    print('Saving Images...')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        \n    for idx, img in enumerate(images):\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n        filename = f'image_{timestamp}_{idx}.png'\n        filepath = os.path.join(output_dir, filename)\n        plt.imsave(filepath, img)\n    print(\"Done!\")\n    return True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, classes) -> None:\n        super(Discriminator, self).__init__()\n        self.classes = classes\n\n        self.embedding = nn.Linear(classes, 1*64*64)\n\n        conv_1 = self.conv_block(4, 64)\n        conv_2 = self.conv_block(64, 128)\n        conv_3 = self.conv_block(128, 256)\n        conv_4 = self.conv_block(256, 512)\n\n        self.classifier = nn.Sequential(\n            conv_1,\n            conv_2,\n            conv_3,\n            conv_4,\n            nn.Conv2d(512, 1024, (5, 5), 2, 1),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Flatten(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 1),\n            nn.Sigmoid()\n        )\n\n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, (5, 5), 2, 2),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n    \n    def forward(self, x, label):\n        label_embedding = self.embedding(label).view(-1, 1, 64, 64)\n        comb_latent_vector = torch.concat((x, label_embedding), dim=1)\n        output = self.classifier(comb_latent_vector)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generator\nclass Generator(nn.Module):\n    def __init__(self, classes):\n        super(Generator, self).__init__()\n        self.classes = classes\n\n        self.embedding = nn.Linear(classes, 8*8)\n\n        self.latent_vector = nn.Sequential(\n            nn.Linear(100, 512*8*8),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        upsample_1 = self.upsample_block(513, 256, 1)\n        upsample_2 = self.upsample_block(256, 128, 1)\n        upsample_3 = self.upsample_block(128, 64, 1)\n\n        self.conv_model = nn.Sequential(\n            upsample_1,\n            upsample_2,\n            upsample_3,\n            nn.Conv2d(64, 3, (1, 1), 1, 0),\n            nn.Tanh()\n        )\n    \n    def upsample_block(self, in_channels, out_channels, padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, (4, 4), 2, padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def forward(self, x, label):\n        latent_vector = self.latent_vector(x).view(-1, 512, 8, 8)\n        label_embedding = self.embedding(label).view(-1, 1, 8, 8)\n        comb_latent_vector = torch.concat((latent_vector, label_embedding), dim = 1)\n        output = self.conv_model(comb_latent_vector)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"manualSeed = 123\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimage_size = 64\nlr = 2e-3\nbeta1 = 0.5\nbatch_size = 64\nnoise_dim = 100\nworkers = 2\nnum_epochs = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Monitor Progress\ndef train(dataloader, classes):\n    progress = list()\n    fixed_noise = torch.randn(classes*10, noise_dim, device=device)\n    fixed_labels = []\n    for i in range(classes):\n        lab = [0 if j != i else 1 for j in range(classes)]\n        lab = lab*10\n        fixed_labels.append(lab)\n    fixed_labels = torch.Tensor(fixed_labels).view(classes*10, classes).float().to(device)\n\n    disc_net = Discriminator(classes)\n    gen_net = Generator(classes)\n    disc_net.to(device)\n    gen_net.to(device)\n    disc_net.apply(weights_init)\n    gen_net.apply(weights_init)\n\n    criterion = nn.BCELoss()\n\n    disc_optimizer = optim.Adam(disc_net.parameters(), lr=lr, betas=(beta1, 0.999))\n    gen_optimizer = optim.Adam(gen_net.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\n    # Training Loop\n\n    # Lists to keep track of progress\n    G_losses = []\n    D_losses = []\n    iters = 0\n    \n    disc_net.train()\n    gen_net.train()\n    print(\"Starting Training Loop...\")\n    for epoch in range(num_epochs):\n        for i, data in enumerate(dataloader, 0):\n            real_images = data[0].to(device)\n            real_labels = data[1].to(device)\n            num_images = real_images.size(0)\n            \n            real_target = torch.ones(num_images,).to(device)\n            fake_target = torch.zeros(num_images,).to(device)\n            \n            # Training the discriminator\n            # Train Discriminator on Real Images and Fake Images\n            disc_net.zero_grad()\n\n            real_output = disc_net(real_images, real_labels).view(-1)\n            disc_err_real = criterion(real_output, real_target)\n            \n            # Conditional Noise\n            noise = torch.randn(num_images, noise_dim, device=device)\n            \n            indices = torch.randint(0, classes, (num_images,))\n            noise_labels = torch.zeros(num_images, classes, device=device)\n            noise_labels[torch.arange(num_images), indices] = 1\n\n            fake = gen_net(noise, noise_labels)\n\n            fake_output = disc_net(fake.detach(), noise_labels).view(-1)\n            disc_err_fake = criterion(fake_output, fake_target)\n\n            disc_err = (disc_err_real + disc_err_fake)/2\n            disc_err.backward()\n            disc_optimizer.step()\n\n            # Training the Generator\n            # Steps:\n            # 1. Get Discriminator Predictions on Fake Images\n            # 2. Calculate loss\n            gen_net.zero_grad()\n            \n            output = disc_net(fake, noise_labels).view(-1)\n\n            gen_err = criterion(output, real_target)\n            gen_err.backward()\n            gen_optimizer.step()\n\n            # Training Update\n            if i % 50 == 0:\n                print(\n                    f\"[{epoch}/{num_epochs}][{i}/{len(dataloader)}]\\tLoss_D: {disc_err.item()}\\tLoss_G: {gen_err.item()}\"\n                )\n\n            # Tracking loss\n            G_losses.append(gen_err.item())\n            D_losses.append(disc_err.item())\n\n            # Tracking Generator Progress\n            if (iters % 10 == 0) or (\n                (epoch == num_epochs - 1) and (i == len(dataloader) - 1)\n            ):\n                gen_net.eval()\n                with torch.no_grad():\n                    fake = gen_net(fixed_noise, fixed_labels).detach().cpu()\n                progress.append(torch_utils.make_grid(fake, padding=2, nrow=10, normalize=True))\n                gen_net.train()\n            iters += 1\n            \n    return gen_net, G_losses, D_losses, progress","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval(classes, model_path, num_images, output_dir):\n    fixed_noise = torch.randn(classes*10, noise_dim, device=device)\n    fixed_labels = []\n    for i in range(classes):\n        lab = [0 if j != i else 1 for j in range(classes)]\n        lab = lab*10\n        fixed_labels.append(lab)\n    fixed_labels = torch.Tensor(fixed_labels).view(classes*10, classes).float().to(device)\n\n    gen_net = Generator(classes)\n    gen_net.to(device)\n    gen_net.load_state_dict(torch.load(model_path))\n\n    iters = 0\n    \n    gen_net.eval()\n    images = []\n    print(\"Starting Inference Loop...\")\n    for image in range(num_images):\n        with torch.no_grad():\n            fake = gen_net(fixed_noise, fixed_labels).detach().cpu()\n        images.append(torch_utils.make_grid(fake, padding=2, nrow=10, normalize=True))\n         \n    return save_images(images, output_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset 1: CelebA","metadata":{}},{"cell_type":"code","source":"class CelebADataset(Dataset):\n    def __init__(self, image_dir, labels_csv, transform=None):\n        \"\"\"\n        Args:\n            image_dir (string): Directory with all the images.\n            labels_csv (string): Path to the csv file with annotations.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.image_dir = image_dir\n        self.labels = pd.read_csv(labels_csv)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.image_dir, self.labels.iloc[idx, 0])\n        image = Image.open(img_name)\n        labels = self.labels.iloc[idx, 1:].to_numpy()\n        labels[labels == -1] = 0\n        labels = labels.astype('float32')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, torch.tensor(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\nlabel_file = \"/kaggle/input/celeba-dataset/list_attr_celeba.csv\"\nmodel_save_path = \"/kaggle/working/cgan_celeba.pt\"\nanimation_save_path = \"/kaggle/working/cgan_celeba.mp4\"\ntraining_plot_save_path = \"/kaggle/working/cgan_celeba.png\"\n\nceleba_dataset = CelebADataset(image_dir = data_dir, \n                               labels_csv = label_file,\n                               transform = transforms.Compose(\n                                [\n                                    transforms.Resize(image_size),\n                                    transforms.CenterCrop(image_size),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                                ]\n                            )\n                            )\n\nceleba_dataloader = data.DataLoader(\n    celeba_dataset, batch_size=batch_size, shuffle=True, num_workers=workers\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"celeba_gen_net, celeba_G_losses, celeba_D_losses, celeba_progress = train(celeba_dataloader, 40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save generator\ntorch.save(celeba_gen_net, model_save_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Training Graph\nfig1 = plt.figure(figsize=(10, 5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(celeba_G_losses, label=\"G\")\nplt.plot(celeba_D_losses, label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(training_plot_save_path)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Progress Animation\nfig2 = plt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in celeba_progress]\nanim = animation.ArtistAnimation(fig2, ims, interval=1000, repeat_delay=1000, blit=True)\nwritervideo = animation.FFMpegWriter(fps=5)\nanim.save(animation_save_path, writer=writervideo)\nplt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval(40, '/kaggle/working/cgan_celeba.pt', 5, '/kaggle/working/celeba_eval')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset 2: Flower Dataset","metadata":{}},{"cell_type":"code","source":"def target_to_oh_flower(target):\n    NUM_CLASS = 5\n    one_hot = torch.eye(NUM_CLASS)[target]\n    return one_hot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/flower-classification-5-classes-roselilyetc/Flower Classification/Flower Classification/Training Data\"\nmodel_save_path = \"/kaggle/working/cgan_flowers.pt\"\nanimation_save_path = \"/kaggle/working/cgan_flowers.mp4\"\ntraining_plot_save_path = \"/kaggle/working/cgan_flowers.png\"\n\nfloower_dataset = datasets.ImageFolder(\n    root=data_dir,\n    transform=transforms.Compose(\n        [\n            transforms.Resize(image_size),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    ),\n    target_transform = target_to_oh_flower\n)\n\nflower_dataloader = data.DataLoader(\n    floower_dataset, batch_size=batch_size, shuffle=True, num_workers=workers\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flower_gen_net, flower_G_losses, flower_D_losses, flower_progress = train(flower_dataloader, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save generator\ntorch.save(flower_gen_net, model_save_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Training Graph\nfig1 = plt.figure(figsize=(10, 5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(flower_G_losses, label=\"G\")\nplt.plot(flower_D_losses, label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(training_plot_save_path)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Progress Animation\nfig2 = plt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in flower_progress]\nanim = animation.ArtistAnimation(fig2, ims, interval=1000, repeat_delay=1000, blit=True)\nwritervideo = animation.FFMpegWriter(fps=5)\nanim.save(animation_save_path, writer=writervideo)\nplt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval(5, '/kaggle/working/cgan_flower.pt', 5, '/kaggle/working/flower_eval')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset 3: Shoe, Sandal, Boot","metadata":{}},{"cell_type":"code","source":"def target_to_oh_shoe(target):\n    NUM_CLASS = 3\n    one_hot = torch.eye(NUM_CLASS)[target]\n    return one_hot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset\"\nmodel_save_path = \"/kaggle/working/cgan_shoe.pt\"\nanimation_save_path = \"/kaggle/working/cgan_shoe.mp4\"\ntraining_plot_save_path = \"/kaggle/working/cgan_shoe.png\"\n\nshoe_dataset = datasets.ImageFolder(\n    root=data_dir,\n    transform=transforms.Compose(\n        [\n            transforms.Resize(image_size),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    ),\n    target_transform = target_to_oh_shoe\n)\n\nshoe_dataloader = data.DataLoader(\n    shoe_dataset, batch_size=batch_size, shuffle=True, num_workers=workers\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shoe_gen_net, shoe_G_losses, shoe_D_losses, shoe_progress = train(shoe_dataloader, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save generator\ntorch.save(shoe_gen_net, model_save_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Training Graph\nfig1 = plt.figure(figsize=(10, 5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(shoe_G_losses, label=\"G\")\nplt.plot(shoe_D_losses, label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(training_plot_save_path)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Progress Animation\nfig2 = plt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in shoe_progress]\nanim = animation.ArtistAnimation(fig2, ims, interval=1000, repeat_delay=1000, blit=True)\nwritervideo = animation.FFMpegWriter(fps=5)\nanim.save(animation_save_path, writer=writervideo)\nplt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval(3, '/kaggle/working/cgan_shoe.pt', 5, '/kaggle/working/shoe_eval')","metadata":{},"execution_count":null,"outputs":[]}]}