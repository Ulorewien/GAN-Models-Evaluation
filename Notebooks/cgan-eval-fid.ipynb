{"cells":[{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:18.236734Z","iopub.status.busy":"2024-06-12T02:23:18.236354Z","iopub.status.idle":"2024-06-12T02:23:18.243181Z","shell.execute_reply":"2024-06-12T02:23:18.242257Z","shell.execute_reply.started":"2024-06-12T02:23:18.236705Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import random\n","from datetime import datetime\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torchvision.utils as torch_utils\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms, models\n","import numpy as np\n","from PIL import Image\n","from scipy.linalg import sqrtm\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:18.671941Z","iopub.status.busy":"2024-06-12T02:23:18.671615Z","iopub.status.idle":"2024-06-12T02:23:18.682165Z","shell.execute_reply":"2024-06-12T02:23:18.681248Z","shell.execute_reply.started":"2024-06-12T02:23:18.671916Z"},"trusted":true},"outputs":[],"source":["# Generator\n","class Generator(nn.Module):\n","    def __init__(self, classes):\n","        super(Generator, self).__init__()\n","        self.classes = classes\n","\n","        self.embedding = nn.Linear(classes, 8*8)\n","\n","        self.latent_vector = nn.Sequential(\n","            nn.Linear(100, 512*8*8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","        )\n","\n","        upsample_1 = self.upsample_block(513, 256, 1)\n","        upsample_2 = self.upsample_block(256, 128, 1)\n","        upsample_3 = self.upsample_block(128, 64, 1)\n","\n","        self.conv_model = nn.Sequential(\n","            upsample_1,\n","            upsample_2,\n","            upsample_3,\n","            nn.Conv2d(64, 3, (1, 1), 1, 0),\n","            nn.Tanh()\n","        )\n","    \n","    def upsample_block(self, in_channels, out_channels, padding):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(in_channels, out_channels, (4, 4), 2, padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","    \n","    def forward(self, x, label):\n","        latent_vector = self.latent_vector(x).view(-1, 512, 8, 8)\n","        label_embedding = self.embedding(label).view(-1, 1, 8, 8)\n","        comb_latent_vector = torch.concat((latent_vector, label_embedding), dim = 1)\n","        output = self.conv_model(comb_latent_vector)\n","        return output"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:19.131880Z","iopub.status.busy":"2024-06-12T02:23:19.131542Z","iopub.status.idle":"2024-06-12T02:23:19.140187Z","shell.execute_reply":"2024-06-12T02:23:19.139238Z","shell.execute_reply.started":"2024-06-12T02:23:19.131854Z"},"trusted":true},"outputs":[],"source":["def save_images(images, output_dir):\n","    print('Saving Images...')\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","        \n","    for idx, img in enumerate(images):\n","        img = img.permute(1, 2, 0)\n","        img = img.numpy()\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n","        filename = f'image_{timestamp}_{idx}.png'\n","        filepath = os.path.join(output_dir, filename)\n","        plt.imsave(filepath, img)\n","    print(\"Done!\")\n","    return True\n","\n","def load_images_as_tensors(image_paths):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),  # Convert PIL Image to Tensor\n","    ])\n","    \n","    image_tensors = []\n","    \n","    for path in image_paths:\n","        image = Image.open(path).convert('RGB')  # Open image and convert to RGB\n","        image_tensors.append(image)\n","    \n","    return image_tensors"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:19.536615Z","iopub.status.busy":"2024-06-12T02:23:19.535704Z","iopub.status.idle":"2024-06-12T02:23:19.543407Z","shell.execute_reply":"2024-06-12T02:23:19.542402Z","shell.execute_reply.started":"2024-06-12T02:23:19.536582Z"},"trusted":true},"outputs":[],"source":["def sample_images_from_directories(folder_path, sample_size=10):\n","    sampled_image_paths = []\n","    \n","    # List all directories in the given folder\n","    for root, dirs, files in os.walk(folder_path):\n","        for dir_name in dirs:\n","            dir_path = os.path.join(root, dir_name)\n","            # List all files in the directory\n","            all_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n","            # Sample 10 files from the list\n","            sampled_files = random.sample(all_files, min(sample_size, len(all_files)))\n","            # Add sampled file paths to the result list\n","            sampled_image_paths.extend(sampled_files)\n","    \n","    return sampled_image_paths"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:20.008826Z","iopub.status.busy":"2024-06-12T02:23:20.008496Z","iopub.status.idle":"2024-06-12T02:23:20.018562Z","shell.execute_reply":"2024-06-12T02:23:20.017587Z","shell.execute_reply.started":"2024-06-12T02:23:20.008803Z"},"trusted":true},"outputs":[],"source":["def eval(classes, model_path, num_images, output_dir, grid=True):\n","    fixed_noise = torch.randn(classes*10, 100, device=device)\n","    fixed_labels = []\n","    for i in range(classes):\n","        lab = [0 if j != i else 1 for j in range(classes)]\n","        lab = lab*10\n","        fixed_labels.append(lab)\n","    fixed_labels = torch.Tensor(fixed_labels).view(classes*10, classes).float().to(device)\n","\n","    gen_net = Generator(classes)\n","    gen_net.to(device)\n","    gen_net.load_state_dict(torch.load(model_path))\n","\n","    iters = 0\n","    \n","    gen_net.eval()\n","    images = []\n","    print(\"Starting Inference Loop...\")\n","    for image in range(num_images):\n","        with torch.no_grad():\n","            fake = gen_net(fixed_noise, fixed_labels).detach().cpu()\n","        \n","        if grid:\n","            images.append(torch_utils.make_grid(fake, padding=2, nrow=10, normalize=True))\n","        else:\n","            for i in range(fake.size(0)):\n","                img = fake[i, :, :, :].squeeze(0)* 255\n","                images.append(img.type(torch.uint8))\n","             \n","    return save_images(images, output_dir)"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:20.432656Z","iopub.status.busy":"2024-06-12T02:23:20.432005Z","iopub.status.idle":"2024-06-12T02:23:20.443121Z","shell.execute_reply":"2024-06-12T02:23:20.442248Z","shell.execute_reply.started":"2024-06-12T02:23:20.432627Z"},"trusted":true},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","]) \n","\n","# Define a custom dataset class\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_tensors):\n","        self.image_tensors = image_tensors\n","\n","    def __len__(self):\n","        return len(self.image_tensors)\n","\n","    def __getitem__(self, idx):\n","        return self.image_tensors[idx], 0  # Dummy label\n","\n","# Function to extract features using InceptionV3\n","def get_features(dataloader, model, device):\n","    model.eval()\n","    features = []\n","    with torch.no_grad():\n","        for images, _ in dataloader:\n","            images = images.to(device)\n","            pred = model(images)\n","            features.extend(pred.cpu().numpy())\n","    return np.array(features)\n","\n","# Function to calculate FID\n","def calculate_fid(real_features, generated_features):\n","    # calculate mean and covariance statistics\n","    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n","    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n","    # calculate sum squared difference between means\n","    ssdiff = np.sum((mu1 - mu2)**2)\n","    # calculate sqrt of product between cov\n","    covmean = sqrtm(sigma1.dot(sigma2))\n","    # check and correct imaginary numbers from sqrt\n","    if np.iscomplexobj(covmean):\n","        covmean = covmean.real\n","    # calculate score\n","    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n","    return fid"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:21.679329Z","iopub.status.busy":"2024-06-12T02:23:21.678910Z","iopub.status.idle":"2024-06-12T02:23:22.078535Z","shell.execute_reply":"2024-06-12T02:23:22.077529Z","shell.execute_reply.started":"2024-06-12T02:23:21.679286Z"},"trusted":true},"outputs":[],"source":["# Load pre-trained InceptionV3 model\n","inception_model = models.inception_v3(weights='DEFAULT', transform_input=False).to(device)\n","inception_model.fc = torch.nn.Identity()  # Remove the final classification layer"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:22.616603Z","iopub.status.busy":"2024-06-12T02:23:22.616232Z","iopub.status.idle":"2024-06-12T02:23:22.718314Z","shell.execute_reply":"2024-06-12T02:23:22.717287Z","shell.execute_reply.started":"2024-06-12T02:23:22.616574Z"},"trusted":true},"outputs":[],"source":["shutil.rmtree('/kaggle/working/shoes_eval')"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:23.163762Z","iopub.status.busy":"2024-06-12T02:23:23.163049Z","iopub.status.idle":"2024-06-12T02:23:28.148600Z","shell.execute_reply":"2024-06-12T02:23:28.147611Z","shell.execute_reply.started":"2024-06-12T02:23:23.163732Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Inference Loop...\n","Saving Images...\n","Done!\n"]},{"data":{"text/plain":["True"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["eval(3, '/kaggle/input/cgan-training/cgan_shoe.pt', 100, '/kaggle/working/shoes_eval', False)"]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:28.150342Z","iopub.status.busy":"2024-06-12T02:23:28.150061Z","iopub.status.idle":"2024-06-12T02:23:50.485409Z","shell.execute_reply":"2024-06-12T02:23:50.484605Z","shell.execute_reply.started":"2024-06-12T02:23:28.150319Z"},"trusted":true},"outputs":[],"source":["# Example usage:\n","dataset_path = '/kaggle/input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset'\n","generated_path = '/kaggle/working/shoes_eval'\n","real_images = load_images_as_tensors(sample_images_from_directories(dataset_path, 300))\n","generated_images = load_images_as_tensors([os.path.join(generated_path, x) for x in os.listdir(generated_path)])"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:23:50.486727Z","iopub.status.busy":"2024-06-12T02:23:50.486474Z","iopub.status.idle":"2024-06-12T02:24:09.252801Z","shell.execute_reply":"2024-06-12T02:24:09.251560Z","shell.execute_reply.started":"2024-06-12T02:23:50.486704Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CGAN Shoes FID Score: 24.234657\n"]}],"source":["real_images = torch.stack([transform(img) for img in real_images])\n","generated_images = torch.stack([transform(img) for img in generated_images])\n","\n","real_dataset = CustomImageDataset(real_images)\n","generated_dataset = CustomImageDataset(generated_images)\n","\n","real_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\n","generated_loader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n","\n","# Extract features\n","real_features = get_features(real_loader, inception_model, device)\n","generated_features = get_features(generated_loader, inception_model, device)\n","\n","# Calculate FID\n","fid_score = calculate_fid(real_features, generated_features)\n","print(f\"CGAN Shoes FID Score: {fid_score}\")"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:24:09.257758Z","iopub.status.busy":"2024-06-12T02:24:09.256406Z","iopub.status.idle":"2024-06-12T02:24:09.267374Z","shell.execute_reply":"2024-06-12T02:24:09.266264Z","shell.execute_reply.started":"2024-06-12T02:24:09.257714Z"},"trusted":true},"outputs":[],"source":["shutil.rmtree('/kaggle/working/flowers_eval')"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:24:09.270239Z","iopub.status.busy":"2024-06-12T02:24:09.269409Z","iopub.status.idle":"2024-06-12T02:24:13.960821Z","shell.execute_reply":"2024-06-12T02:24:13.959935Z","shell.execute_reply.started":"2024-06-12T02:24:09.270183Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Inference Loop...\n","Saving Images...\n","Done!\n"]},{"data":{"text/plain":["True"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["eval(5, '/kaggle/input/cgan-training/cgan_flowers.pt', 50, '/kaggle/working/flowers_eval', False)"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:24:13.962195Z","iopub.status.busy":"2024-06-12T02:24:13.961929Z","iopub.status.idle":"2024-06-12T02:24:44.840621Z","shell.execute_reply":"2024-06-12T02:24:44.839618Z","shell.execute_reply.started":"2024-06-12T02:24:13.962172Z"},"trusted":true},"outputs":[],"source":["dataset_path = '/kaggle/input/flower-classification-5-classes-roselilyetc/Flower Classification/Flower Classification'\n","generated_path = '/kaggle/working/flowers_eval'\n","real_images = load_images_as_tensors(sample_images_from_directories(dataset_path, 500))\n","generated_images = load_images_as_tensors([os.path.join(generated_path, x) for x in os.listdir(generated_path)])"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:24:44.842245Z","iopub.status.busy":"2024-06-12T02:24:44.841879Z","iopub.status.idle":"2024-06-12T02:25:21.248137Z","shell.execute_reply":"2024-06-12T02:25:21.247157Z","shell.execute_reply.started":"2024-06-12T02:24:44.842196Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CGAN Flowers FID Score: 30.6876865\n"]}],"source":["real_images = torch.stack([transform(img) for img in real_images])\n","generated_images = torch.stack([transform(img) for img in generated_images])\n","\n","real_dataset = CustomImageDataset(real_images)\n","generated_dataset = CustomImageDataset(generated_images)\n","\n","real_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\n","generated_loader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n","\n","# Extract features\n","real_features = get_features(real_loader, inception_model, device)\n","generated_features = get_features(generated_loader, inception_model, device)\n","\n","# Calculate FID\n","fid_score = calculate_fid(real_features, generated_features)\n","print(f\"CGAN Flowers FID Score: {fid_score}\")"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:25:21.250217Z","iopub.status.busy":"2024-06-12T02:25:21.249529Z","iopub.status.idle":"2024-06-12T02:25:21.608843Z","shell.execute_reply":"2024-06-12T02:25:21.607854Z","shell.execute_reply.started":"2024-06-12T02:25:21.250167Z"},"trusted":true},"outputs":[],"source":["shutil.rmtree('/kaggle/working/mnist_eval')"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:25:21.610632Z","iopub.status.busy":"2024-06-12T02:25:21.610177Z","iopub.status.idle":"2024-06-12T02:25:42.565142Z","shell.execute_reply":"2024-06-12T02:25:42.564239Z","shell.execute_reply.started":"2024-06-12T02:25:21.610606Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Inference Loop...\n","Saving Images...\n","Done!\n"]},{"data":{"text/plain":["True"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["eval(10, '/kaggle/input/cgan-training/cgan_mnist.pt', 100, '/kaggle/working/mnist_eval', False)"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:25:42.568609Z","iopub.status.busy":"2024-06-12T02:25:42.568331Z","iopub.status.idle":"2024-06-12T02:27:05.554997Z","shell.execute_reply":"2024-06-12T02:27:05.554212Z","shell.execute_reply.started":"2024-06-12T02:25:42.568586Z"},"trusted":true},"outputs":[],"source":["dataset_path = '/kaggle/input/mnistasjpg/trainingSet/trainingSet'\n","generated_path = '/kaggle/working/mnist_eval'\n","real_images = load_images_as_tensors(sample_images_from_directories(dataset_path, 1000))\n","generated_images = load_images_as_tensors([os.path.join(generated_path, x) for x in os.listdir(generated_path)])"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T02:27:05.556276Z","iopub.status.busy":"2024-06-12T02:27:05.555990Z","iopub.status.idle":"2024-06-12T02:28:12.756910Z","shell.execute_reply":"2024-06-12T02:28:12.755440Z","shell.execute_reply.started":"2024-06-12T02:27:05.556249Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CGAN MNIST FID Score: 2.4678997\n"]}],"source":["real_images = torch.stack([transform(img) for img in real_images])\n","generated_images = torch.stack([transform(img) for img in generated_images])\n","\n","real_dataset = CustomImageDataset(real_images)\n","generated_dataset = CustomImageDataset(generated_images)\n","\n","real_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\n","generated_loader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n","\n","# Extract features\n","real_features = get_features(real_loader, inception_model, device)\n","generated_features = get_features(generated_loader, inception_model, device)\n","\n","# Calculate FID\n","fid_score = calculate_fid(real_features, generated_features)\n","print(f\"CGAN MNIST FID Score: {fid_score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1272,"sourceId":2280,"sourceType":"datasetVersion"},{"datasetId":2490389,"sourceId":4418815,"sourceType":"datasetVersion"},{"datasetId":2604803,"sourceId":4607952,"sourceType":"datasetVersion"},{"sourceId":181320290,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
