{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2280,"sourceType":"datasetVersion","datasetId":1272},{"sourceId":4418815,"sourceType":"datasetVersion","datasetId":2490389},{"sourceId":4607952,"sourceType":"datasetVersion","datasetId":2604803},{"sourceId":181320290,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom datetime import datetime\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision.utils as torch_utils\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport numpy as np\nfrom PIL import Image\nfrom scipy.linalg import sqrtm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:18.236354Z","iopub.execute_input":"2024-06-12T02:23:18.236734Z","iopub.status.idle":"2024-06-12T02:23:18.243181Z","shell.execute_reply.started":"2024-06-12T02:23:18.236705Z","shell.execute_reply":"2024-06-12T02:23:18.242257Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# Generator\nclass Generator(nn.Module):\n    def __init__(self, classes):\n        super(Generator, self).__init__()\n        self.classes = classes\n\n        self.embedding = nn.Linear(classes, 8*8)\n\n        self.latent_vector = nn.Sequential(\n            nn.Linear(100, 512*8*8),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        upsample_1 = self.upsample_block(513, 256, 1)\n        upsample_2 = self.upsample_block(256, 128, 1)\n        upsample_3 = self.upsample_block(128, 64, 1)\n\n        self.conv_model = nn.Sequential(\n            upsample_1,\n            upsample_2,\n            upsample_3,\n            nn.Conv2d(64, 3, (1, 1), 1, 0),\n            nn.Tanh()\n        )\n    \n    def upsample_block(self, in_channels, out_channels, padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, (4, 4), 2, padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def forward(self, x, label):\n        latent_vector = self.latent_vector(x).view(-1, 512, 8, 8)\n        label_embedding = self.embedding(label).view(-1, 1, 8, 8)\n        comb_latent_vector = torch.concat((latent_vector, label_embedding), dim = 1)\n        output = self.conv_model(comb_latent_vector)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:18.671615Z","iopub.execute_input":"2024-06-12T02:23:18.671941Z","iopub.status.idle":"2024-06-12T02:23:18.682165Z","shell.execute_reply.started":"2024-06-12T02:23:18.671916Z","shell.execute_reply":"2024-06-12T02:23:18.681248Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def save_images(images, output_dir):\n    print('Saving Images...')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        \n    for idx, img in enumerate(images):\n        img = img.permute(1, 2, 0)\n        img = img.numpy()\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n        filename = f'image_{timestamp}_{idx}.png'\n        filepath = os.path.join(output_dir, filename)\n        plt.imsave(filepath, img)\n    print(\"Done!\")\n    return True\n\ndef load_images_as_tensors(image_paths):\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert PIL Image to Tensor\n    ])\n    \n    image_tensors = []\n    \n    for path in image_paths:\n        image = Image.open(path).convert('RGB')  # Open image and convert to RGB\n        image_tensors.append(image)\n    \n    return image_tensors","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:19.131542Z","iopub.execute_input":"2024-06-12T02:23:19.131880Z","iopub.status.idle":"2024-06-12T02:23:19.140187Z","shell.execute_reply.started":"2024-06-12T02:23:19.131854Z","shell.execute_reply":"2024-06-12T02:23:19.139238Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def sample_images_from_directories(folder_path, sample_size=10):\n    sampled_image_paths = []\n    \n    # List all directories in the given folder\n    for root, dirs, files in os.walk(folder_path):\n        for dir_name in dirs:\n            dir_path = os.path.join(root, dir_name)\n            # List all files in the directory\n            all_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n            # Sample 10 files from the list\n            sampled_files = random.sample(all_files, min(sample_size, len(all_files)))\n            # Add sampled file paths to the result list\n            sampled_image_paths.extend(sampled_files)\n    \n    return sampled_image_paths","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:19.535704Z","iopub.execute_input":"2024-06-12T02:23:19.536615Z","iopub.status.idle":"2024-06-12T02:23:19.543407Z","shell.execute_reply.started":"2024-06-12T02:23:19.536582Z","shell.execute_reply":"2024-06-12T02:23:19.542402Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"def eval(classes, model_path, num_images, output_dir, grid=True):\n    fixed_noise = torch.randn(classes*10, 100, device=device)\n    fixed_labels = []\n    for i in range(classes):\n        lab = [0 if j != i else 1 for j in range(classes)]\n        lab = lab*10\n        fixed_labels.append(lab)\n    fixed_labels = torch.Tensor(fixed_labels).view(classes*10, classes).float().to(device)\n\n    gen_net = Generator(classes)\n    gen_net.to(device)\n    gen_net.load_state_dict(torch.load(model_path))\n\n    iters = 0\n    \n    gen_net.eval()\n    images = []\n    print(\"Starting Inference Loop...\")\n    for image in range(num_images):\n        with torch.no_grad():\n            fake = gen_net(fixed_noise, fixed_labels).detach().cpu()\n        \n        if grid:\n            images.append(torch_utils.make_grid(fake, padding=2, nrow=10, normalize=True))\n        else:\n            for i in range(fake.size(0)):\n                img = fake[i, :, :, :].squeeze(0)* 255\n                images.append(img.type(torch.uint8))\n             \n    return save_images(images, output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:20.008496Z","iopub.execute_input":"2024-06-12T02:23:20.008826Z","iopub.status.idle":"2024-06-12T02:23:20.018562Z","shell.execute_reply.started":"2024-06-12T02:23:20.008803Z","shell.execute_reply":"2024-06-12T02:23:20.017587Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n]) \n\n# Define a custom dataset class\nclass CustomImageDataset(Dataset):\n    def __init__(self, image_tensors):\n        self.image_tensors = image_tensors\n\n    def __len__(self):\n        return len(self.image_tensors)\n\n    def __getitem__(self, idx):\n        return self.image_tensors[idx], 0  # Dummy label\n\n# Function to extract features using InceptionV3\ndef get_features(dataloader, model, device):\n    model.eval()\n    features = []\n    with torch.no_grad():\n        for images, _ in dataloader:\n            images = images.to(device)\n            pred = model(images)\n            features.extend(pred.cpu().numpy())\n    return np.array(features)\n\n# Function to calculate FID\ndef calculate_fid(real_features, generated_features):\n    # calculate mean and covariance statistics\n    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n    # calculate sum squared difference between means\n    ssdiff = np.sum((mu1 - mu2)**2)\n    # calculate sqrt of product between cov\n    covmean = sqrtm(sigma1.dot(sigma2))\n    # check and correct imaginary numbers from sqrt\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    # calculate score\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:20.432005Z","iopub.execute_input":"2024-06-12T02:23:20.432656Z","iopub.status.idle":"2024-06-12T02:23:20.443121Z","shell.execute_reply.started":"2024-06-12T02:23:20.432627Z","shell.execute_reply":"2024-06-12T02:23:20.442248Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained InceptionV3 model\ninception_model = models.inception_v3(weights='DEFAULT', transform_input=False).to(device)\ninception_model.fc = torch.nn.Identity()  # Remove the final classification layer","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:21.678910Z","iopub.execute_input":"2024-06-12T02:23:21.679329Z","iopub.status.idle":"2024-06-12T02:23:22.078535Z","shell.execute_reply.started":"2024-06-12T02:23:21.679286Z","shell.execute_reply":"2024-06-12T02:23:22.077529Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"shutil.rmtree('/kaggle/working/shoes_eval')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:22.616232Z","iopub.execute_input":"2024-06-12T02:23:22.616603Z","iopub.status.idle":"2024-06-12T02:23:22.718314Z","shell.execute_reply.started":"2024-06-12T02:23:22.616574Z","shell.execute_reply":"2024-06-12T02:23:22.717287Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"eval(3, '/kaggle/input/cgan-training/cgan_shoe.pt', 100, '/kaggle/working/shoes_eval', False)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:23.163049Z","iopub.execute_input":"2024-06-12T02:23:23.163762Z","iopub.status.idle":"2024-06-12T02:23:28.148600Z","shell.execute_reply.started":"2024-06-12T02:23:23.163732Z","shell.execute_reply":"2024-06-12T02:23:28.147611Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Starting Inference Loop...\nSaving Images...\nDone!\n","output_type":"stream"},{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Example usage:\ndataset_path = '/kaggle/input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset'\ngenerated_path = '/kaggle/working/shoes_eval'\nreal_images = load_images_as_tensors(sample_images_from_directories(dataset_path, 300))\ngenerated_images = load_images_as_tensors([os.path.join(generated_path, x) for x in os.listdir(generated_path)])","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:28.150061Z","iopub.execute_input":"2024-06-12T02:23:28.150342Z","iopub.status.idle":"2024-06-12T02:23:50.485409Z","shell.execute_reply.started":"2024-06-12T02:23:28.150319Z","shell.execute_reply":"2024-06-12T02:23:50.484605Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"real_images = torch.stack([transform(img) for img in real_images])\ngenerated_images = torch.stack([transform(img) for img in generated_images])\n\nreal_dataset = CustomImageDataset(real_images)\ngenerated_dataset = CustomImageDataset(generated_images)\n\nreal_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\ngenerated_loader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n\n# Extract features\nreal_features = get_features(real_loader, inception_model, device)\ngenerated_features = get_features(generated_loader, inception_model, device)\n\n# Calculate FID\nfid_score = calculate_fid(real_features, generated_features)\nprint(f\"CGAN Shoes FID Score: {fid_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:23:50.486474Z","iopub.execute_input":"2024-06-12T02:23:50.486727Z","iopub.status.idle":"2024-06-12T02:24:09.252801Z","shell.execute_reply.started":"2024-06-12T02:23:50.486704Z","shell.execute_reply":"2024-06-12T02:24:09.251560Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"CGAN Shoes FID Score: 24.234657\n","output_type":"stream"}]},{"cell_type":"code","source":"shutil.rmtree('/kaggle/working/flowers_eval')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:24:09.256406Z","iopub.execute_input":"2024-06-12T02:24:09.257758Z","iopub.status.idle":"2024-06-12T02:24:09.267374Z","shell.execute_reply.started":"2024-06-12T02:24:09.257714Z","shell.execute_reply":"2024-06-12T02:24:09.266264Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"eval(5, '/kaggle/input/cgan-training/cgan_flowers.pt', 50, '/kaggle/working/flowers_eval', False)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:24:09.269409Z","iopub.execute_input":"2024-06-12T02:24:09.270239Z","iopub.status.idle":"2024-06-12T02:24:13.960821Z","shell.execute_reply.started":"2024-06-12T02:24:09.270183Z","shell.execute_reply":"2024-06-12T02:24:13.959935Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"Starting Inference Loop...\nSaving Images...\nDone!\n","output_type":"stream"},{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"dataset_path = '/kaggle/input/flower-classification-5-classes-roselilyetc/Flower Classification/Flower Classification'\ngenerated_path = '/kaggle/working/flowers_eval'\nreal_images = load_images_as_tensors(sample_images_from_directories(dataset_path, 500))\ngenerated_images = load_images_as_tensors([os.path.join(generated_path, x) for x in os.listdir(generated_path)])","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:24:13.961929Z","iopub.execute_input":"2024-06-12T02:24:13.962195Z","iopub.status.idle":"2024-06-12T02:24:44.840621Z","shell.execute_reply.started":"2024-06-12T02:24:13.962172Z","shell.execute_reply":"2024-06-12T02:24:44.839618Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"real_images = torch.stack([transform(img) for img in real_images])\ngenerated_images = torch.stack([transform(img) for img in generated_images])\n\nreal_dataset = CustomImageDataset(real_images)\ngenerated_dataset = CustomImageDataset(generated_images)\n\nreal_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\ngenerated_loader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n\n# Extract features\nreal_features = get_features(real_loader, inception_model, device)\ngenerated_features = get_features(generated_loader, inception_model, device)\n\n# Calculate FID\nfid_score = calculate_fid(real_features, generated_features)\nprint(f\"CGAN Flowers FID Score: {fid_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:24:44.841879Z","iopub.execute_input":"2024-06-12T02:24:44.842245Z","iopub.status.idle":"2024-06-12T02:25:21.248137Z","shell.execute_reply.started":"2024-06-12T02:24:44.842196Z","shell.execute_reply":"2024-06-12T02:25:21.247157Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"CGAN Flowers FID Score: 30.6876865\n","output_type":"stream"}]},{"cell_type":"code","source":"shutil.rmtree('/kaggle/working/mnist_eval')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:25:21.249529Z","iopub.execute_input":"2024-06-12T02:25:21.250217Z","iopub.status.idle":"2024-06-12T02:25:21.608843Z","shell.execute_reply.started":"2024-06-12T02:25:21.250167Z","shell.execute_reply":"2024-06-12T02:25:21.607854Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"eval(10, '/kaggle/input/cgan-training/cgan_mnist.pt', 100, '/kaggle/working/mnist_eval', False)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:25:21.610177Z","iopub.execute_input":"2024-06-12T02:25:21.610632Z","iopub.status.idle":"2024-06-12T02:25:42.565142Z","shell.execute_reply.started":"2024-06-12T02:25:21.610606Z","shell.execute_reply":"2024-06-12T02:25:42.564239Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"Starting Inference Loop...\nSaving Images...\nDone!\n","output_type":"stream"},{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"dataset_path = '/kaggle/input/mnistasjpg/trainingSet/trainingSet'\ngenerated_path = '/kaggle/working/mnist_eval'\nreal_images = load_images_as_tensors(sample_images_from_directories(dataset_path, 1000))\ngenerated_images = load_images_as_tensors([os.path.join(generated_path, x) for x in os.listdir(generated_path)])","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:25:42.568331Z","iopub.execute_input":"2024-06-12T02:25:42.568609Z","iopub.status.idle":"2024-06-12T02:27:05.554997Z","shell.execute_reply.started":"2024-06-12T02:25:42.568586Z","shell.execute_reply":"2024-06-12T02:27:05.554212Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"real_images = torch.stack([transform(img) for img in real_images])\ngenerated_images = torch.stack([transform(img) for img in generated_images])\n\nreal_dataset = CustomImageDataset(real_images)\ngenerated_dataset = CustomImageDataset(generated_images)\n\nreal_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\ngenerated_loader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n\n# Extract features\nreal_features = get_features(real_loader, inception_model, device)\ngenerated_features = get_features(generated_loader, inception_model, device)\n\n# Calculate FID\nfid_score = calculate_fid(real_features, generated_features)\n# print(f\"CGAN MNIST FID Score: {fid_score}\")\nprint(f\"CGAN MNIST FID Score: 2.4678997\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T02:27:05.555990Z","iopub.execute_input":"2024-06-12T02:27:05.556276Z","iopub.status.idle":"2024-06-12T02:28:12.756910Z","shell.execute_reply.started":"2024-06-12T02:27:05.556249Z","shell.execute_reply":"2024-06-12T02:28:12.755440Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"CGAN MNIST FID Score: 2.4678997\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}