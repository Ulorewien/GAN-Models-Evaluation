{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "# Generator and Discriminator Utility Functions\n",
    "def initialize_linear_layer(layer):\n",
    "    nn.init.xavier_normal(layer.weight)\n",
    "    layer.bias.data.zero_()\n",
    "\n",
    "def initialize_conv_layer(layer):\n",
    "    nn.init.kaiming_normal(layer.weight)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.zero_()\n",
    "        \n",
    "def equal_layer(module, name=\"weight\"):\n",
    "    EqualLR.apply(module, name)\n",
    "    return module\n",
    "\n",
    "\n",
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + \"_orig\")\n",
    "        input = weight.data.size(1) * weight.data[0][0].numel()\n",
    "        return weight * math.sqrt(2 / input)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "        weight = getattr(module, name)\n",
    "        del module._parameters[name]\n",
    "        module.register_parameter(name + \"_orig\", nn.Parameter(weight.data))\n",
    "        module.register_forward_pre_hook(fn)\n",
    "        return fn\n",
    "\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "class EqualConvLayer(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        conv = nn.Conv2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        self.conv = equal_layer(conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class EqualLinearLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        linear = nn.Linear(in_channels, out_channels)\n",
    "        linear.weight.data.normal_()\n",
    "        linear.bias.data.zero_()\n",
    "        self.linear = equal_layer(linear)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, kernel_size2=None, padding2=None):\n",
    "        super().__init__()\n",
    "        pad1 = padding\n",
    "        pad2 = padding\n",
    "        if padding2 is not None:\n",
    "            pad2 = padding2\n",
    "\n",
    "        kernel1 = kernel_size\n",
    "        kernel2 = kernel_size\n",
    "        if kernel_size2 is not None:\n",
    "            kernel2 = kernel_size2\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            EqualConvLayer(in_channels, out_channels, kernel1, pad1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualConvLayer(out_channels, out_channels, kernel2, pad2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, in_channels, style_dim):\n",
    "        super().__init__()\n",
    "        self.adain = nn.InstanceNorm2d(in_channels)\n",
    "        self.style = EqualLinearLayer(style_dim, in_channels*2)\n",
    "        self.style.linear.bias.data[:in_channels] = 1\n",
    "        self.style.linear.bias.data[in_channels:] = 0\n",
    "\n",
    "    def forward(self, x, style):\n",
    "        style = self.style(style).unsqueeze(2).unsqueeze(3)\n",
    "        gamma, beta = style.chunk(2, 1)\n",
    "        x = self.adain(x)\n",
    "        x = gamma*x + beta\n",
    "        return x\n",
    "\n",
    "class AddNoise(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        return x + self.weight*noise\n",
    "\n",
    "class ConstantIn(nn.Module):\n",
    "    def __init__(self, channels, size=4):\n",
    "        super().__init__()\n",
    "        self.input = nn.Parameter(torch.randn(1, channels, size, size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        out = self.input.repeat(batch, 1, 1, 1)\n",
    "        return out\n",
    "\n",
    "class StyleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, style_dim=512, initial=False):\n",
    "        super().__init__()\n",
    "        if initial:\n",
    "            self.conv1 = ConstantIn(in_channels)\n",
    "        else:\n",
    "            self.conv1 = EqualConvLayer(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.noise1 = equal_layer(AddNoise(out_channels))\n",
    "        self.adain1 = AdaIN(out_channels, style_dim)\n",
    "        self.leakyrelu1 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv2 = EqualConvLayer(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.noise2 = equal_layer(AddNoise(out_channels))\n",
    "        self.adain2 = AdaIN(out_channels, style_dim)\n",
    "        self.leakyrelu2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, style, noise):\n",
    "        x = self.conv1(x)\n",
    "        x = self.noise1(x, noise)\n",
    "        x = self.adain1(x, style)\n",
    "        x = self.leakyrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.noise2(x, noise)\n",
    "        x = self.adain2(x, style)\n",
    "        x = self.leakyrelu2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Training Utility Functions\n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "\n",
    "def get_data_sample(dataset, batch_size, image_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    dataset.transform = transform\n",
    "    return DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=16)\n",
    "\n",
    "def adjust_lr(optimizer, lr):\n",
    "    for group in optimizer.param_groups:\n",
    "        group[\"lr\"] = lr * group.get(\"mult\", 1)\n",
    "\n",
    "# Shoe Dataset\n",
    "def target_to_oh_shoe(target):\n",
    "    NUM_CLASS = 3\n",
    "    one_hot = torch.eye(NUM_CLASS)[target]\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10, condition=2):\n",
    "        super().__init__()\n",
    "        if type(condition) not in (list, tuple):\n",
    "            condition = [condition]\n",
    "        condition_channels = [num_classes if i in condition else 0 for i in range(9)]\n",
    "        self.progression_list = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(16, 32, 3, 1),\n",
    "                ConvBlock(32, 64, 3, 1),\n",
    "                ConvBlock(64, 128, 3, 1),\n",
    "                ConvBlock(128, 256, 3, 1),\n",
    "                ConvBlock(256, 512, 3, 1),\n",
    "                ConvBlock(512, 512, 3, 1),\n",
    "                ConvBlock(512, 512, 3, 1),\n",
    "                ConvBlock(512, 512, 3, 1),\n",
    "                ConvBlock(512, 512, 3, 1, 4, 0),\n",
    "            ]\n",
    "        )\n",
    "        self.rgb_list = nn.ModuleList(\n",
    "            [\n",
    "                EqualConvLayer(3+condition_channels[8], 16, 1),\n",
    "                EqualConvLayer(3+condition_channels[7], 32, 1),\n",
    "                EqualConvLayer(3+condition_channels[6], 64, 1),\n",
    "                EqualConvLayer(3+condition_channels[5], 128, 1),\n",
    "                EqualConvLayer(3+condition_channels[4], 256, 1),\n",
    "                EqualConvLayer(3+condition_channels[3], 512, 1),\n",
    "                EqualConvLayer(3+condition_channels[2], 512, 1),\n",
    "                EqualConvLayer(3+condition_channels[1], 512, 1),\n",
    "                EqualConvLayer(3+condition_channels[0], 512, 1),\n",
    "            ]\n",
    "        )\n",
    "        self.num_layers = len(self.progression_list)\n",
    "        self.equal_linear = EqualLinearLayer(512, 1)\n",
    "        self.condition = condition\n",
    "        self.num_classes = num_classes\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x, label, step=0, alpha=-1):\n",
    "        label = self.label_emb(label).view(-1, self.num_classes, 1, 1)\n",
    "\n",
    "        for i in range(step, -1, -1):\n",
    "            index = self.num_layers - i - 1\n",
    "            downsample_input = current_input = x\n",
    "            if i in self.condition:\n",
    "                current_input = torch.cat([x, label.repeat(1, 1, *x.shape[2:])], dim=1)\n",
    "            if i-1 in self.condition:\n",
    "                downsample_input = torch.cat([x, label.repeat(1, 1, *x.shape[2:])], dim=1)\n",
    "            if i == step:\n",
    "                out = self.rgb_list[index](current_input)\n",
    "            if i == 0:\n",
    "                out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)\n",
    "                mean_std = out_std.mean()\n",
    "                mean_std = mean_std.expand(out.size(0), 1, 4, 4)\n",
    "                out = torch.cat([out, mean_std], 1)\n",
    "\n",
    "            out = self.progression_list[index](out)\n",
    "            if i > 0:\n",
    "                out = F.interpolate(out, scale_factor=0.5, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "                if i == step and 0 <= alpha < 1:\n",
    "                    skip_rgb = self.rgb_list[index + 1](downsample_input)\n",
    "                    skip_rgb = F.interpolate(skip_rgb, scale_factor=0.5, mode=\"bilinear\", align_corners=False)\n",
    "                    out = (1 - alpha)*skip_rgb + alpha*out\n",
    "\n",
    "        out = out.squeeze(2).squeeze(2)\n",
    "        out = self.equal_linear(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, code_dim, num_classes=10, condition=2):\n",
    "        super().__init__()\n",
    "        if type(condition) not in (list, tuple):\n",
    "            condition = [condition]\n",
    "        self.progression_list = nn.ModuleList(\n",
    "            [\n",
    "                StyleConvBlock(512, 512, 3, 1, initial=True),\n",
    "                StyleConvBlock(512, 512, 3, 1),\n",
    "                StyleConvBlock(512, 512, 3, 1),\n",
    "                StyleConvBlock(512, 512, 3, 1),\n",
    "                StyleConvBlock(512, 256, 3, 1),\n",
    "                StyleConvBlock(256, 128, 3, 1),\n",
    "                StyleConvBlock(128, 64, 3, 1),\n",
    "                StyleConvBlock(64, 32, 3, 1),\n",
    "                StyleConvBlock(32, 16, 3, 1),\n",
    "            ]\n",
    "        )\n",
    "        self.rgb_list = nn.ModuleList(\n",
    "            [\n",
    "                EqualConvLayer(512, 3, 1),\n",
    "                EqualConvLayer(512, 3, 1),\n",
    "                EqualConvLayer(512, 3, 1),\n",
    "                EqualConvLayer(512, 3, 1),\n",
    "                EqualConvLayer(256, 3, 1),\n",
    "                EqualConvLayer(128, 3, 1),\n",
    "                EqualConvLayer(64, 3, 1),\n",
    "                EqualConvLayer(32, 3, 1),\n",
    "                EqualConvLayer(16, 3, 1),\n",
    "            ]\n",
    "        )\n",
    "        self.condition = condition\n",
    "        self.num_classes = num_classes\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "    def forward(self, style, label, noise, step=0, alpha=-1, mixing_range=(-1, -1)):\n",
    "        out = noise[0]\n",
    "        if len(style) < 2:\n",
    "            inject_index = [len(self.progression_list) + 1]\n",
    "        else:\n",
    "            inject_index = random.sample(list(range(step)), len(style) - 1)\n",
    "\n",
    "        crossover = 0\n",
    "        for i, (conv, to_rgb) in enumerate(zip(self.progression_list, self.rgb_list)):\n",
    "            if mixing_range == (-1, -1):\n",
    "                if crossover < len(inject_index) and i > inject_index[crossover]:\n",
    "                    crossover = min(crossover + 1, len(style))\n",
    "                style_step = style[crossover]\n",
    "            else:\n",
    "                if mixing_range[0] <= i <= mixing_range[1]:\n",
    "                    style_step = style[1]\n",
    "                else:\n",
    "                    style_step = style[0]\n",
    "\n",
    "            style_step = style_step.clone()\n",
    "            if i in self.condition:\n",
    "                style_step[:, :self.num_classes] = self.label_emb(label).view(-1, self.num_classes)\n",
    "            else:\n",
    "                style_step[:, :self.num_classes] = torch.zeros(style_step.shape[0], self.num_classes).to(style_step.device)\n",
    "\n",
    "            if i > 0 and step > 0:\n",
    "                upsample = F.interpolate(out, scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "                out = conv(upsample, style_step, noise[i])\n",
    "            else:\n",
    "                out = conv(out, style_step, noise[i])\n",
    "\n",
    "            if i == step:\n",
    "                out = to_rgb(out)\n",
    "                if i > 0 and 0 <= alpha < 1:\n",
    "                    skip_rgb = self.rgb_list[i - 1](upsample)\n",
    "                    out = (1 - alpha)*skip_rgb + alpha*out\n",
    "                break\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class StyledGenerator(nn.Module):\n",
    "    def __init__(self, code_dim=512, num_mlp=8, num_classes=10, condition=2):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(code_dim, num_classes=num_classes, condition=condition)\n",
    "        layers = [PixelNorm()]\n",
    "        for _ in range(num_mlp):\n",
    "            layers.append(EqualLinearLayer(code_dim, code_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        self.style = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, label, noise=None, step=0, alpha=-1, mean_style=None, style_weight=0, mixing_range=(-1, -1),):\n",
    "        styles = []\n",
    "        if type(x) not in (list, tuple):\n",
    "            x = [x]\n",
    "\n",
    "        for i in x:\n",
    "            styles.append(self.style(i))\n",
    "\n",
    "        batch = x[0].shape[0]\n",
    "        if noise is None:\n",
    "            noise = []\n",
    "            for i in range(step + 1):\n",
    "                size = 4 * 2 ** i\n",
    "                noise.append(torch.randn(batch, 1, size, size, device=x[0].device))\n",
    "\n",
    "        if mean_style is not None:\n",
    "            styles_norm = []\n",
    "            for style in styles:\n",
    "                styles_norm.append(mean_style + style_weight * (style - mean_style))\n",
    "            styles = styles_norm\n",
    "\n",
    "        return self.generator(styles, label, noise, step, alpha, mixing_range=mixing_range)\n",
    "\n",
    "    def mean_style(self, x):\n",
    "        style = self.style(x).mean(0, keepdim=True)\n",
    "        return style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "from torch import optim\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train(args, dataset, generator, discriminator):\n",
    "    step = int(math.log2(args.init_size)) - 2\n",
    "    resolution = 4 * 2 ** step\n",
    "    loader = get_data_sample(dataset, args.batch.get(resolution, args.batch_default), resolution)\n",
    "    data_loader = iter(loader)\n",
    "\n",
    "    adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
    "    adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
    "    pbar = tqdm(range(3_000_000), mininterval=30, maxinterval=60)\n",
    "    requires_grad(generator, False)\n",
    "    requires_grad(discriminator, True)\n",
    "\n",
    "    disc_loss_val = 0\n",
    "    gen_loss_val = 0\n",
    "    grad_loss_val = 0\n",
    "    alpha = 0\n",
    "    used_sample = 0\n",
    "\n",
    "    for i in pbar:\n",
    "        discriminator.zero_grad()\n",
    "        alpha = min(1, 1 / args.phase * (used_sample + 1))\n",
    "\n",
    "        if used_sample > args.phase * 2:\n",
    "            step += 1\n",
    "            if step > int(math.log2(args.max_size)) - 2:\n",
    "                step = int(math.log2(args.max_size)) - 2\n",
    "            else:\n",
    "                alpha = 0\n",
    "                used_sample = 0\n",
    "\n",
    "            resolution = 4 * 2 ** step\n",
    "            loader = get_data_sample(dataset, args.batch.get(resolution, args.batch_default), resolution)\n",
    "            data_loader = iter(loader)\n",
    "            torch.save(\n",
    "                {\n",
    "                    'generator': generator.module.state_dict(),\n",
    "                    'discriminator': discriminator.module.state_dict(),\n",
    "                    'g_optimizer': g_optimizer.state_dict(),\n",
    "                    'd_optimizer': d_optimizer.state_dict(),\n",
    "                },\n",
    "                f'checkpoint/train_step-{step}.model',\n",
    "            )\n",
    "            adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
    "            adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
    "\n",
    "        try:\n",
    "            real_image, label = next(data_loader)\n",
    "        except (OSError, StopIteration):\n",
    "            data_loader = iter(loader)\n",
    "            real_image, label = next(data_loader)\n",
    "\n",
    "        used_sample += real_image.shape[0]\n",
    "        b_size = real_image.size(0)\n",
    "        real_image = real_image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        if args.loss == 'wgan-gp':\n",
    "            real_predict, label_predict = discriminator(real_image, step=step, alpha=alpha)\n",
    "            real_predict = real_predict.mean() - 0.001 * (real_predict ** 2).mean()\n",
    "            (-real_predict).backward()\n",
    "        elif args.loss == 'r1':\n",
    "            real_image.requires_grad = True\n",
    "            real_predict, label_predict = discriminator(real_image, step=step, alpha=alpha)\n",
    "            real_predict = F.softplus(-real_predict).mean()\n",
    "            real_predict.backward(retain_graph=True)\n",
    "            grad_real = grad(outputs=real_predict.sum(), inputs=real_image, create_graph=True)[0]\n",
    "            grad_penalty = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()\n",
    "            grad_penalty = 10 / 2 * grad_penalty\n",
    "            grad_penalty.backward()\n",
    "            grad_loss_val = grad_penalty.item()\n",
    "\n",
    "        if args.mixing and random.random() < 0.9:\n",
    "            gen_in11, gen_in12, gen_in21, gen_in22 = torch.randn(4, b_size, code_size, device=device).chunk(4, 0)\n",
    "            gen_in1 = [gen_in11.squeeze(0), gen_in12.squeeze(0)]\n",
    "            gen_in2 = [gen_in21.squeeze(0), gen_in22.squeeze(0)]\n",
    "        else:\n",
    "            gen_in1, gen_in2 = torch.randn(2, b_size, code_size, device=device).chunk(2, 0)\n",
    "            gen_in1 = gen_in1.squeeze(0)\n",
    "            gen_in2 = gen_in2.squeeze(0)\n",
    "\n",
    "        fake_image = generator(gen_in1, step=step, alpha=alpha)\n",
    "        fake_predict, _ = discriminator(fake_image, step=step, alpha=alpha)\n",
    "\n",
    "        if args.loss == 'wgan-gp':\n",
    "            fake_predict = fake_predict.mean()\n",
    "            fake_predict.backward()\n",
    "            eps = torch.rand(b_size, 1, 1, 1).to(device)\n",
    "            x_hat = eps * real_image.data + (1 - eps) * fake_image.data\n",
    "            x_hat.requires_grad = True\n",
    "            hat_predict, _ = discriminator(x_hat, step=step, alpha=alpha)\n",
    "            grad_x_hat = grad(outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\n",
    "            grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1).norm(2, dim=1) - 1) ** 2).mean()\n",
    "            grad_penalty = 10 * grad_penalty\n",
    "            grad_penalty.backward()\n",
    "            grad_loss_val = grad_penalty.item()\n",
    "            disc_loss_val = (real_predict - fake_predict).item()\n",
    "        elif args.loss == 'r1':\n",
    "            fake_predict = F.softplus(fake_predict).mean()\n",
    "            fake_predict.backward()\n",
    "            disc_loss_val = (real_predict + fake_predict).item()\n",
    "\n",
    "        d_optimizer.step()\n",
    "\n",
    "        if (i + 1) % n_critic == 0:\n",
    "            generator.zero_grad()\n",
    "            requires_grad(generator, True)\n",
    "            requires_grad(discriminator, False)\n",
    "            fake_image = generator(gen_in2, step=step, alpha=alpha)\n",
    "            predict, _ = discriminator(fake_image, step=step, alpha=alpha)\n",
    "\n",
    "            if args.loss == 'wgan-gp':\n",
    "                loss = -predict.mean()\n",
    "            elif args.loss == 'r1':\n",
    "                loss = F.softplus(-predict).mean()\n",
    "\n",
    "            gen_loss_val = loss.item()\n",
    "            loss.backward()\n",
    "            g_optimizer.step()\n",
    "            accumulate(g_running, generator.module)\n",
    "            requires_grad(generator, False)\n",
    "            requires_grad(discriminator, True)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            images = []\n",
    "            gen_i, gen_j = args.gen_sample.get(resolution, (10, 5))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _ in range(gen_i):\n",
    "                    images.append(g_running(torch.randn(gen_j, code_size).to(device), step=step, alpha=alpha).data.cpu())\n",
    "\n",
    "            utils.save_image(\n",
    "                torch.cat(images, 0),\n",
    "                f'sample/{str(i + 1).zfill(6)}.png',\n",
    "                nrow=gen_i,\n",
    "                normalize=True,\n",
    "                range=(-1, 1),\n",
    "            )\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            torch.save(g_running.state_dict(), f'checkpoint/{str(i + 1).zfill(6)}.model')\n",
    "        if (i + 1) % 100 == 0:\n",
    "            state_msg = (\n",
    "                f'Size: {4 * 2 ** step}; G: {gen_loss_val:.3f}; D: {disc_loss_val:.3f};'\n",
    "                f' Grad: {grad_loss_val:.3f}; Alpha: {alpha:.5f}'\n",
    "            )\n",
    "            pbar.set_description(state_msg)\n",
    "\n",
    "\n",
    "code_size = 512\n",
    "batch_size = 16\n",
    "n_critic = 1\n",
    "image_size = 64\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Progressive Growing of GANs\")\n",
    "\n",
    "parser.add_argument(\"--path\", type=str, default=\"data\", help=\"path of specified dataset\")\n",
    "parser.add_argument(\"--phase\", type=int, default=600_000, help=\"number of samples used for each training phase\")\n",
    "parser.add_argument(\"--lr\", default=0.001, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--sched\", action=\"store_true\", help=\"use lr scheduling\")\n",
    "parser.add_argument(\"--init_size\", default=64, type=int, help=\"initial image size\")\n",
    "parser.add_argument(\"--max_size\", default=1024, type=int, help=\"max image size\")\n",
    "parser.add_argument(\"--mixing\", action=\"store_true\", help=\"use mixing regularization\")\n",
    "parser.add_argument(\"--loss\", type=str, default=\"wgan-gp\", choices=[\"wgan-gp\", \"r1\"], help=\"class of gan loss\")\n",
    "parser.add_argument(\"-d\", \"--data\", default=\"shoe\", type=str, choices=[\"shoe\", \"lsun\"], help=(\"Specify dataset.\" \"Currently Shoe Dataset is supported\"))\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "generator = nn.DataParallel(StyledGenerator(code_size)).to(device)\n",
    "discriminator = nn.DataParallel(Discriminator()).to(device)\n",
    "g_running = StyledGenerator(code_size).to(device)\n",
    "g_running.train(False)\n",
    "\n",
    "class_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "g_optimizer = optim.Adam(generator.module.generator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "g_optimizer.add_param_group(\n",
    "    {\n",
    "        \"params\": generator.module.style.parameters(),\n",
    "        \"lr\": args.lr * 0.01,\n",
    "        \"mult\": 0.01,\n",
    "    }\n",
    ")\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "\n",
    "accumulate(g_running, generator.module, 0)\n",
    "\n",
    "if args.data == \"shoe\":\n",
    "    data_dir = \"/kaggle/input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset\"\n",
    "\n",
    "    dataset = datasets.ImageFolder(\n",
    "        root=data_dir,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.CenterCrop(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "        target_transform = target_to_oh_shoe\n",
    "    )\n",
    "\n",
    "if args.sched:\n",
    "    args.lr = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n",
    "    args.batch = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32, 128: 32, 256: 32}\n",
    "else:\n",
    "    args.lr = {}\n",
    "    args.batch = {}\n",
    "\n",
    "args.gen_sample = {512: (8, 4), 1024: (4, 2)}\n",
    "\n",
    "args.batch_default = 32\n",
    "\n",
    "train(args, dataset, generator, discriminator)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
