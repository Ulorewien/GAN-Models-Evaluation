{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torchvision.utils as torch_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator Utilities\n",
    "class WSConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "    \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# Train and Eval utilities\n",
    "def generate_examples(gen, steps, z_dim, n=100):\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(1, z_dim).to(device)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            if not os.path.exists(f'saved_examples/step{steps}'):\n",
    "                os.makedirs(f'saved_examples/step{steps}')\n",
    "            save_image(img*0.5+0.5, f\"saved_examples/step{steps}/img_{i}.png\")\n",
    "    gen.train()\n",
    "\n",
    "  \n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    " \n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def get_loader(image_size, channels_img, batch_sizes, dataset_dir):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((image_size, image_size)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.RandomHorizontalFlip(p=0.5),\n",
    "         transforms.Normalize(\n",
    "            [0.5 for _ in range(channels_img)],\n",
    "            [0.5 for _ in range(channels_img)],\n",
    "         )\n",
    "        ]\n",
    "    )\n",
    "    batch_size = batch_sizes[int(math.log2(image_size/4))]\n",
    "    dataset = datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return loader, dataset\n",
    "\n",
    "def check_loader():\n",
    "    loader, _ = get_loader(128)\n",
    "    cloth, _  = next(iter(loader))\n",
    "    _, ax     = plt.subplots(3,3,figsize=(8,8))\n",
    "    plt.suptitle('Some real samples')\n",
    "    ind = 0\n",
    "    for k in range(3):\n",
    "        for kk in range(3):\n",
    "            ax[k][kk].imshow((cloth[ind].permute(1,2,0)+1)/2)\n",
    "            ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n",
    "\n",
    "# Normalization on every element of input vector\n",
    "# Adapted from StyleGAN original Implementation\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)\n",
    "    \n",
    "# Implementing the Noise Mapping Network\n",
    "class WSLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features, out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.scale = (2 / in_features)**0.5\n",
    "        self.bias = self.linear.bias\n",
    "        self.linear.bias = None\n",
    "\n",
    "        # initialize linear layer\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x * self.scale) + self.bias\n",
    "    \n",
    "\n",
    "class NoiseMappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.noise_mapping = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            WSLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.noise_mapping(x)\n",
    "    \n",
    "\n",
    "# Adaptive Instance Normalization (AdaIn)\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale = WSLinear(w_dim, channels)\n",
    "        self.style_bias = WSLinear(w_dim, channels)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.instance_norm(x)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * x + style_bias\n",
    "    \n",
    "\n",
    "class NoiseInjectNet(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device=x.device)\n",
    "        return x + self.weight * noise\n",
    "\n",
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, w_dim):\n",
    "        super(GenBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.inject_noise1 = NoiseInjectNet(out_channels)\n",
    "        self.inject_noise2 = NoiseInjectNet(out_channels)\n",
    "        self.adain1 = AdaIN(out_channels, w_dim)\n",
    "        self.adain2 = AdaIN(out_channels, w_dim)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.adain1(self.leaky(self.inject_noise1(self.conv1(x))), w)\n",
    "        x = self.adain2(self.leaky(self.inject_noise2(self.conv2(x))), w)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, in_channels, img_channels=3, classes=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Linear(classes, 4*4)\n",
    "\n",
    "        self.starting_constant = nn.Parameter(torch.ones((1, in_channels, 4, 4)))\n",
    "        self.map = NoiseMappingNetwork(z_dim, w_dim)\n",
    "        self.initial_adain1 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_adain2 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_noise1 = NoiseInjectNet(in_channels)\n",
    "        self.initial_noise2 = NoiseInjectNet(in_channels)\n",
    "        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(len(image_factors) - 1):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_channels * image_factors[i])\n",
    "            conv_out_c = int(in_channels * image_factors[i + 1])\n",
    "            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, noise, label, alpha, steps):\n",
    "#         print(label.shape)\n",
    "        w = self.map(noise)\n",
    "        x = self.initial_adain1(self.initial_noise1(self.starting_constant), w)\n",
    "        label_embedding = self.embedding(label).view(-1, 1, 4, 4)\n",
    "        factor = x.shape[-1]//label_embedding.shape[-1]\n",
    "        a, b, c, d = label_embedding.shape\n",
    "        label_embedding = label_embedding.view(a, b, c, 1, d, 1)\n",
    "        label_embedding = label_embedding.repeat(1, 1, 1, factor, 1, factor)\n",
    "        label_embedding = label_embedding.reshape(a, b, x.shape[-1], x.shape[-1])\n",
    "#         print(x.shape, label_embedding.shape)\n",
    "#         x = torch.concat((x, label_embedding), dim = 1)\n",
    "        x = x + label_embedding\n",
    "        x = self.initial_conv(x)\n",
    "        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(x)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"bilinear\")\n",
    "            out = self.prog_blocks[step](upscaled, w)\n",
    "\n",
    "        # The number of channels in upscale will stay the same, while\n",
    "        # out which has moved through prog_blocks might change. To ensure\n",
    "        # we can convert both to rgb we use different rgb_layers\n",
    "        # (steps-1) and steps for upscaled, out respectively\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __main__\n",
    "setattr(__main__, \"Generator\", Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "z_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, output_dir):\n",
    "    print('Saving Images...')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for idx, img in enumerate(images):\n",
    "        img = img.permute(1, 2, 0)\n",
    "        img = img.numpy()\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "        filename = f'image_{timestamp}_{idx}.png'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.imsave(filepath, img)\n",
    "    print(\"Done!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(classes, model_path, num_images, output_dir):\n",
    "    fixed_noise = torch.randn(classes*10, z_dim, device=device)\n",
    "    fixed_labels = []\n",
    "    for i in range(classes):\n",
    "        lab = [0 if j != i else 1 for j in range(classes)]\n",
    "        lab = lab*10\n",
    "        fixed_labels.append(lab)\n",
    "    fixed_labels = torch.Tensor(fixed_labels).view(classes*10, classes).float().to(device)\n",
    "\n",
    "    gen_net = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    gen_net.eval()\n",
    "    images = []\n",
    "    print(\"Starting Inference Loop...\")\n",
    "    for _ in range(num_images):\n",
    "        with torch.no_grad():\n",
    "            fake = gen_net(fixed_noise, fixed_labels, 1, 4).detach().cpu()\n",
    "        images.append(torch_utils.make_grid(fake, padding=2, nrow=10, normalize=True))\n",
    "         \n",
    "    return save_images(images, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Inference Loop...\n",
      "Saving Images...\n",
      "Done!\n",
      "Starting Inference Loop...\n",
      "Saving Images...\n",
      "Done!\n",
      "Starting Inference Loop...\n",
      "Saving Images...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(10, \"..\\\\Outputs\\\\StyleGAN\\\\models\\\\stylegan_mnist.pth\", 5, \"Plots1\")\n",
    "eval(5, \"..\\\\Outputs\\\\StyleGAN\\\\models\\\\stylegan_flowers.pth\", 5, \"Plots2\")\n",
    "eval(3, \"..\\\\Outputs\\\\StyleGAN\\\\models\\\\stylegan_shoe.pth\", 5, \"Plots3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
